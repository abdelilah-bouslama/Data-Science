{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark-and-RDD(part1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1P58CZqDpNdyXKbQUp6p6mUpLEvsxSu6P",
      "authorship_tag": "ABX9TyMoY0yIy+IC9QgILLYls7v7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdelilah-bouslama/Data-Science/blob/master/Apache_Spark/Spark_and_RDD(part1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uko7F1FYvpP",
        "colab_type": "text"
      },
      "source": [
        "Flowing youtube chanel :  https://www.youtube.com/watch?v=CoYIwoeQxMY&list=PLot-YkcC7wZ_2sxmRTZr2c121rjcaleqv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X0qzR60YcJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# INSTALLING SPARK\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMGwlH5AZN8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DEFINING SYSTEM VARIABLES\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTcJAhvnZgpE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUbuohaygZXN",
        "colab_type": "text"
      },
      "source": [
        "# **EXAMPLE 1: WORD COUNT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxQerDFjbEUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rdd = spark.sparkContext.textFile(\"/content/drive/My Drive/BIG DATA/pyspark/Tutorial-2/datasets/word_count.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukpAI_Z-hEkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NOT COMPLETE YET\n",
        "words = rdd.map(lambda x: x.split(\" \"))\n",
        "words = words.count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3v3K5DzgegK",
        "colab_type": "text"
      },
      "source": [
        "# **EXAMPLE 2: Airplans USA Problems**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq-0BvJognHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rdd = spark.sparkContext.textFile(\"/content/drive/My Drive/BIG DATA/pyspark/Tutorial-2/datasets/airports.text\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EdCTS6ChDLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = rdd.map(lambda line: line.split(\",\")) \\\n",
        "    .filter(lambda line: line[3] == \"\\\"United States\\\"\") \\\n",
        "    .map(lambda line: \"{},{}\".format(line[1], line[2])) \\\n",
        "    .collect()\n",
        "result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAPVmCftl3hT",
        "colab_type": "text"
      },
      "source": [
        "# **EXAMPLE 3 : : Aiplans , LATITUTE > 40**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw9fLjP4atNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "class Utils():\n",
        "    COMMA_DELIMITER = re.compile(''',(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)''')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F4L4duol_m6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = rdd.map(lambda line: Utils.COMMA_DELIMITER.split(line)) \\\n",
        "    .filter(lambda line : float(line[6]) > 40) \\\n",
        "    .map(lambda line : \"{},{}\".format(line[1], line[6])) \\\n",
        "    .collect()\n",
        "result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLI9ElmxhYf5",
        "colab_type": "text"
      },
      "source": [
        "# **EXAMPLE 4 :  (1) Union Operation **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsknoDlXhD9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "firstDataset = '/content/drive/My Drive/BIG DATA/pyspark/Tutorial-2/datasets/union/nasa_19950701.tsv.txt'\n",
        "secondDataSet = '/content/drive/My Drive/BIG DATA/pyspark/Tutorial-2/datasets/union/nasa_19950801.tsv.txt'\n",
        "firstRdd = spark.sparkContext.textFile(firstDataset)\n",
        "secondRdd = spark.sparkContext.textFile(secondDataSet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMgYc1PskX6r",
        "colab_type": "code",
        "outputId": "083b5f3c-9c9e-4d8f-cc93-f300d807a263",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "fullRdd = firstRdd.union(secondRdd)\n",
        "filtredRdd = fullRdd.filter(lambda line : not (line.startsWith(\"host\") and \"bytes\"))\n",
        "sampleRdd = filtredRdd.sample(withReplacement = True, fraction = 0.1)\n",
        "sampleRdd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[47] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0ah8bDCk4WV",
        "colab_type": "text"
      },
      "source": [
        "# **EXAMPLE 4 :  (2) Intersection Operation **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpevLxIjk848",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Not working\n",
        "fullRdd = firstRdd.intersection(secondRdd)\n",
        "filtredRdd = fullRdd.filter(lambda line : not (line.startsWith(\"host\") and \"bytes\"))\n",
        "filtredRdd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7kVKxO4owsa",
        "colab_type": "text"
      },
      "source": [
        "# **EXAMPLE 5 :  Reduce **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN5O5SGDo53i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "primeDataset = '/content/drive/My Drive/BIG DATA/pyspark/Tutorial-2/datasets/prime_nums.text'\n",
        "primeRdd = spark.sparkContext.textFile(primeDataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81Xyw15DpDA6",
        "colab_type": "code",
        "outputId": "2406ddae-c6df-4a70-cbd5-ea26845d97a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "prime_sum = primeRdd.flatMap(lambda line : line.split(\"\\t\")) \\\n",
        "    .filter(lambda number: number) \\\n",
        "    .map(lambda number: int(number)) \\\n",
        "    .reduce(lambda x, y: x + y)\n",
        "prime_sum"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24133"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    }
  ]
}